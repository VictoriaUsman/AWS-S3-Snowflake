import boto3
import pandas as pd
from datetime import datetime
from io import StringIO
from boto3.dynamodb.types import TypeDeserializer

# Initialize outside the handler for better performance
s3 = boto3.client('s3')
deserializer = TypeDeserializer()

def lambda_handler(event, context):
    # 1. Safety Check: Ensure 'Records' exists
    if 'Records' not in event:
        print("No Records found in event.")
        return

    processed_rows = []
    table_name = "unknown_table"

    # 2. Loop through every record in the batch
    for record in event['Records']:
        # Get table name from the ARN
        table_name = record['eventSourceARN'].split("/")[1]

        # 3. Only capture INSERTs
        if record['eventName'] == "INSERT":
            new_image = record['dynamodb'].get('NewImage', {})
            
            # Convert DynamoDB JSON to standard Python dict
            # This handles strings, numbers, and booleans correctly
            row = {k: deserializer.deserialize(v) for k, v in new_image.items()}
            
            # Add this row to our list instead of overwriting a variable
            processed_rows.append(row)

    # 4. Process the gathered rows
    if processed_rows:
        # Create one DataFrame containing ALL records
        df = pd.DataFrame(processed_rows)
        df = df.astype(str) # Ensure string format for CSV safety

        # Create unique filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        file_key = f"snowflake/{table_name}_{timestamp}.csv"
        
        # Upload to S3
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)
        
        s3.put_object(
            Bucket="iantristan-weatherbucket",
            Key=file_key,
            Body=csv_buffer.getvalue()
        )
        
        print(f"Successfully saved {len(processed_rows)} records to {file_key}")
    else:
        print("No INSERT records found in this batch.")
